{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T11:48:06.254178Z",
     "start_time": "2020-11-08T11:48:06.111005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov  8 19:48:06 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:0A:00.0  On |                  N/A |\r\n",
      "|  0%   54C    P2   194W / 300W |  10201MiB / 11016MiB |     77%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:0B:00.0 Off |                  N/A |\r\n",
      "|  0%   52C    P2   197W / 260W |  10614MiB / 11019MiB |     85%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1276      G   /usr/lib/xorg/Xorg                            18MiB |\r\n",
      "|    0      1607      G   /usr/bin/gnome-shell                          57MiB |\r\n",
      "|    0      1961      G   /usr/lib/xorg/Xorg                           141MiB |\r\n",
      "|    0      2089      G   /usr/bin/gnome-shell                         127MiB |\r\n",
      "|    0     11647      C   python                                      9841MiB |\r\n",
      "|    1      7693      C   ...p307-2-4/anaconda3/envs/tu36/bin/python   819MiB |\r\n",
      "|    1     11647      C   python                                      9783MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original datamgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T11:48:08.626386Z",
     "start_time": "2020-11-08T11:48:06.256020Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep307-2-4/anaconda3/envs/tu36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/deep307-2-4/anaconda3/envs/tu36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/deep307-2-4/anaconda3/envs/tu36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/deep307-2-4/anaconda3/envs/tu36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/deep307-2-4/anaconda3/envs/tu36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/deep307-2-4/anaconda3/envs/tu36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from data.datamgr import SetDataManager, DataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T11:48:08.635393Z",
     "start_time": "2020-11-08T11:48:08.627772Z"
    }
   },
   "outputs": [],
   "source": [
    "base_file = 'filelists/omniglot/base.json'\n",
    "image_size = 28\n",
    "n_query = 15\n",
    "few_shot_params = dict(n_way = 3, n_support = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T11:48:09.026898Z",
     "start_time": "2020-11-08T11:48:08.636616Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep307-2-4/anaconda3/envs/tu36/lib/python3.6/site-packages/torchvision/transforms/transforms.py:211: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "base_datamgr = SetDataManager(image_size, n_query = n_query,  **few_shot_params)\n",
    "base_loader = base_datamgr.get_data_loader( base_file , aug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T11:48:13.026235Z",
     "start_time": "2020-11-08T11:48:09.028773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_data[0].type(): torch.FloatTensor\n",
      "first_data[1].type(): torch.LongTensor\n",
      "torch.Size([3, 20, 3, 28, 28])\n",
      "torch.Size([3, 20])\n",
      "torch.FloatTensor\n",
      "torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "first_data = base_loader.dataset[0]\n",
    "print('first_data[0].type():', first_data[0].type())\n",
    "print('first_data[1].type():', first_data[1].type())\n",
    "for i, (x,_ ) in enumerate(base_loader):\n",
    "    print(x.shape)\n",
    "    print(_.shape)\n",
    "    print(x.type())\n",
    "    print(_.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### customized datamgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T11:48:13.031365Z",
     "start_time": "2020-11-08T11:48:13.027954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import torch\n",
    "# import torchvision.transforms as transforms\n",
    "# from data.dataset import EpisodicBatchSampler\n",
    "from data.datamgr import VirtualSetDataManager, load_npz_dataset\n",
    "# from data.dataset import VirtualSetDataset\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T11:48:13.105132Z",
     "start_time": "2020-11-08T11:48:13.032702Z"
    }
   },
   "outputs": [],
   "source": [
    "n_dims = 100\n",
    "n_all_classes = 600 # base max 400 / source_val 100 / source_novel 100\n",
    "n_samples_per_class = 40 # i think 40 is enough\n",
    "n_classes = {\n",
    "    'base':400, 'val':100, 'novel':100, \n",
    "    'base50cl':50, 'base100cl':100, 'base200cl':200,}\n",
    "distrib_center = np.zeros(n_dims)\n",
    "distrib_std = 20 * np.ones(n_dims)\n",
    "# base_distrib_radius = 1\n",
    "# base_x_std = 1\n",
    "cls_x_std = 1\n",
    "\n",
    "n_target_classes = 200\n",
    "n_val_classes = 100\n",
    "n_novel_classes = 100\n",
    "target_distrib_center = 1 * np.ones(n_dims)\n",
    "target_distrib_radius = 1\n",
    "target_x_std = 1\n",
    "target_cls_x_std = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T11:48:13.111109Z",
     "start_time": "2020-11-08T11:48:13.108555Z"
    }
   },
   "outputs": [],
   "source": [
    "n_base_informative = 20 # first 20 features\n",
    "n_target_informative = 50 # first 50 features\n",
    "assert n_base_informative <= n_dims\n",
    "assert n_target_informative <= n_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T11:48:13.126162Z",
     "start_time": "2020-11-08T11:48:13.113113Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "'''\n",
    "base dataset: \n",
    "    different n_classes\n",
    "    (different n_samples_per_class?)\n",
    "    \n",
    "target dataset:\n",
    "    different domain shift\n",
    "'''\n",
    "\n",
    "# def load_dataset(path):\n",
    "#     assert '.npz' in path, 'load path should be .npz file'\n",
    "#     data = np.load(path)\n",
    "#     return data['X'], data['y']\n",
    "\n",
    "class DatasetGenerator:\n",
    "    def __init__(self, n_dims, n_all_classes, n_classes, n_samples_per_class):#, datafolder='./'):\n",
    "        self.n_dims = n_dims\n",
    "        self.n_all_classes = n_all_classes\n",
    "        self.n_classes = n_classes # dictionary base50cl/base400cl/val/novel\n",
    "        self.n_samples_per_class = n_samples_per_class\n",
    "        self.n_samples = n_all_classes * n_samples_per_class\n",
    "#         self.datafolder = datafolder\n",
    "        assert n_all_classes == n_classes['base'] + n_classes['val'] + n_classes['novel']\n",
    "        \n",
    "    def gen_random_dataset(self, save_path=None):\n",
    "        n_samples = self.n_samples_per_class * self.n_all_classes\n",
    "        X_shape = (n_samples, self.n_dims)\n",
    "        X = np.random.random(X_shape)\n",
    "        y = np.repeat(np.arange(self.n_all_classes),self.n_samples_per_class,axis=0) # [0 0 1 1 2 2 ...]\n",
    "        if save_path is not None:\n",
    "            assert '.npz' in save_path, 'save path should be .npz file'\n",
    "            np.savez(save_path, X=X, y=y)#, X_mean=X.mean(axis=0), X_std=X.std(axis=0))\n",
    "        return X, y\n",
    "        \n",
    "    def gen_Gaussian_datasets(self, datafolder, distrib_center, distrib_std, cls_x_std, informative_interval):\n",
    "        ''' generate base100cl, base200cl, base400cl, val, novel datasets\n",
    "        '''\n",
    "        n_informative = informative_interval[1] - informative_interval[0] + 1\n",
    "        distrib_center_info_feat = distrib_center[informative_interval[0]:informative_interval[1]+1]\n",
    "        distrib_std_info_feat = distrib_std[informative_interval[0]:informative_interval[1]+1] # actually no need that complex currently\n",
    "        informative_x_centers = np.random.normal(\n",
    "            loc = distrib_center_info_feat, scale = distrib_std_info_feat, \n",
    "            size = (self.n_all_classes, n_informative)\n",
    "        ) # shape: (n_all_classes, n_informative)\n",
    "#         print(informative_x_centers.shape)\n",
    "        X_info = []\n",
    "        for cl in range(self.n_all_classes):\n",
    "            info_x_center = informative_x_centers[cl]\n",
    "            cl_X_info = np.random.normal(\n",
    "                loc = info_x_center, scale = cls_x_std, \n",
    "                size = (self.n_samples_per_class, n_informative)\n",
    "#                 size = (n_informative, self.n_samples_per_class) # this would get error @@\n",
    "            )\n",
    "            X_info.append(cl_X_info)\n",
    "        X_info = np.concatenate(X_info, axis=0)\n",
    "        X_noninfo_center = distrib_center[:-n_informative] # hack (actually should be dimensions except informative_interval)\n",
    "        X_noninfo_std = distrib_std[:-n_informative] # hack (actually should be dimensions except informative_interval)\n",
    "        X_noninfo = np.random.normal(\n",
    "            loc = X_noninfo_center, scale = X_noninfo_std, \n",
    "            size = (self.n_samples, self.n_dims-n_informative)\n",
    "        )\n",
    "        X1 = X_noninfo[:, :informative_interval[0]] # hack\n",
    "        X2 = X_noninfo[:, informative_interval[0]:] # hack\n",
    "        X_all = np.concatenate(\n",
    "            (X1, X_info, X2)\n",
    "            , axis=1)\n",
    "        y_all = np.repeat(np.arange(self.n_all_classes),self.n_samples_per_class,axis=0) # [0 0 1 1 2 2 ...]\n",
    "        test_dataset_ls = ['val', 'novel']\n",
    "        Xs = {}\n",
    "        ys = {}\n",
    "#         for split in test_dataset_ls:\n",
    "        for split in self.n_classes.keys():\n",
    "            dataset_n_classes = self.n_classes[split]\n",
    "            dataset_n_samples = dataset_n_classes * self.n_samples_per_class\n",
    "            if 'base' in split:\n",
    "                X = X_all[:dataset_n_samples]\n",
    "                y = y_all[:dataset_n_samples]\n",
    "            elif split == 'novel':\n",
    "                X = X_all[-dataset_n_samples:]\n",
    "                y = y_all[-dataset_n_samples:]\n",
    "            elif split == 'val':\n",
    "                novel_n_samples = self.n_classes['novel'] * self.n_samples_per_class\n",
    "                X = X_all[-novel_n_samples-dataset_n_samples:-novel_n_samples]\n",
    "                y = y_all[-novel_n_samples-dataset_n_samples:-novel_n_samples]\n",
    "            else:\n",
    "                raise ValueError('Unknown split: %s'%(split))\n",
    "\n",
    "            Xs[split] = X\n",
    "            ys[split] = y\n",
    "            filename = split + '.npz'\n",
    "            out_path = os.path.join(datafolder, filename)\n",
    "            print('Saving file: %s'%(out_path))\n",
    "            if not os.path.exists(datafolder):\n",
    "                print('Folder not exist: \"%s\"'%(datafolder))\n",
    "                print('Making directory...')\n",
    "                os.makedirs(datafolder)\n",
    "            np.savez(out_path, X=X, y=y)\n",
    "        \n",
    "        return Xs, ys\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T11:48:13.134742Z",
     "start_time": "2020-11-08T11:48:13.127509Z"
    }
   },
   "outputs": [],
   "source": [
    "# filepath = 'filelists/virtual_20info/try_base200cl.npz'\n",
    "\n",
    "datafolder = './'\n",
    "dataset_generator = DatasetGenerator(\n",
    "    n_dims = n_dims, n_all_classes = n_all_classes, n_classes = n_classes, \n",
    "    n_samples_per_class = n_samples_per_class)#, datafolder = datafolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T11:48:13.140392Z",
     "start_time": "2020-11-08T11:48:13.136340Z"
    }
   },
   "outputs": [],
   "source": [
    "should_gen_npy = False\n",
    "if should_gen_npy:\n",
    "    filepath = '000.npz'\n",
    "    X, y = dataset_generator.gen_random_dataset(save_path=file_path)\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    X, y = load_npz_dataset(file_path)\n",
    "    print(X.shape)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T11:48:13.304322Z",
     "start_time": "2020-11-08T11:48:13.142109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datafolder: filelists/virtual_info3059/\n",
      "Saving file: filelists/virtual_info3059/base.npz\n",
      "Folder:\n",
      "Saving file: filelists/virtual_info3059/val.npz\n",
      "Saving file: filelists/virtual_info3059/novel.npz\n",
      "Saving file: filelists/virtual_info3059/base50cl.npz\n",
      "Saving file: filelists/virtual_info3059/base100cl.npz\n",
      "Saving file: filelists/virtual_info3059/base200cl.npz\n"
     ]
    }
   ],
   "source": [
    "should_gen_Gaussian = True\n",
    "informative_interval = (30, 59)\n",
    "info_int_s1 = str(informative_interval[0]).zfill(2)\n",
    "info_int_s2 = str(informative_interval[1]).zfill(2)\n",
    "datafolder = 'filelists/virtual_info%s%s/'%(info_int_s1, info_int_s2)\n",
    "print('datafolder:', datafolder)\n",
    "if should_gen_Gaussian:\n",
    "    dataset_generator.gen_Gaussian_datasets(\n",
    "        datafolder = datafolder, \n",
    "        distrib_center=distrib_center, distrib_std=distrib_std, \n",
    "        cls_x_std=cls_x_std, informative_interval=informative_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T11:48:13.513033Z",
     "start_time": "2020-11-08T11:48:13.305452Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aaa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-972a1a11f199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maaa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'aaa' is not defined"
     ]
    }
   ],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T11:48:13.521993Z",
     "start_time": "2020-11-08T11:48:04.850Z"
    }
   },
   "outputs": [],
   "source": [
    "base_datamgr = VirtualSetDataManager(in_dim = n_dims, n_query = 15,  **few_shot_params)\n",
    "base_loader = base_datamgr.get_data_loader( filepath = filepath, aug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T11:48:13.522472Z",
     "start_time": "2020-11-08T11:48:04.853Z"
    }
   },
   "outputs": [],
   "source": [
    "first_data = base_loader.dataset[0]\n",
    "print('first_data[0].type():', first_data[0].type())\n",
    "print('first_data[1].type():', first_data[1].type())\n",
    "\n",
    "for i, (x,y ) in enumerate(base_loader):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(x.type())\n",
    "    print(y.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tu36]",
   "language": "python",
   "name": "conda-env-tu36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
